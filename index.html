<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <title>Mike Lazos - Home</title>

        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css" />
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">

        <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
        <link rel="stylesheet" type="text/css" href="./css/custom.css" />
    </head>
    <body>
        <nav class="navbar navbar-default navbar-static-top">
            <div class="container-fluid">
                <div class="navbar-header">
                    <a class="navbar-brand" href="#">Mike Lazos</a>
                </div>
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav">
                        <li><a href="./index.html">Home</a></li>
                        <li><a href="./about.html">About</a></li>
                        <li><a href="./projects.html">Projects</a></li>
                        <li><a href="./posts.html">Posts</a></li>
                        <li><a href="./readings.html">Reading List</a></li>
                    </ul>
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="http://github.com/mlazos"><i class="fa fa-github fa-lg fa-fw"></i></a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <div class="container">
            <div class="row">
                <div class="col-md-10 col-md-offset-1">
                    <div class="page-header">
    <h1>Discovering the Kernel Trick</h1>
    <small>Posted on April  2, 2018 <a href="./posts/2018-04-02-Discovering-Kernel-Trick.html"><i class="fa fa-link fa-lg fa-fw"></i></a></small>
</div>

<p>In this post, I’ll explain the motivation of the kernel trick using a minimal example.</p>
<h3 id="introduction">Introduction</h3>
<p>Although the kernel trick can be applied to a variety of pattern recognition algorithms - essentially any that utilizes a similarity function - our focus will be on applying the kernel trick in the context of a classification problem utilizing a simple linear classifier.</p>
<h3 id="background">Background</h3>
<p>If you’re already familiar with the perceptron, then feel free to skip this section.</p>
<h3 id="motivating-example-classifying-pressure-gauges">Motivating Example: Classifying Pressure Gauges</h3>
<p>As a New England Patriots fan, I have in past years become acquainted with a tool that is used throughout modern sports: the pressure gauge. Let’s say an NFL referee has access to two pressure gauges, one which always produces measurements with an error that is less than +/- 5 PSI and another which always produces measurements with an error that is greater than +/- 4 PSI. The referee used both to measure the air pressure of an inflated football but can’t remember which measurements he took with each gauge. In order to conduct an objective investigation this information is important! (Especially to the NFL commissioner) Luckily, he still has access to the gauges and can gather some data to make an educated guess. The referee gathers some measurements and records the error. This yields the following measurements.</p>
<div class="figure">
<img src="./resources/gauge_errors.png" alt="Figure 1: Pressure Gauge Errors Recorded by Referee" style="width:80.0%" />
<p class="caption"><strong>Figure 1</strong>: Pressure Gauge Errors Recorded by Referee</p>
</div>
<p>So we take our linear classifier and try to train it on this data. Unfortunately, there really isn’t a way to have a single linear decision boundary with this data; it’s non-separable. This can be easily seen by simply looking at the data, there really isn’t a way to separate the data into two halves without some significant error. Looking at the structure of the data though, we can augment our error data with an additional feature. Rather than just using the error value itself, we can add another dimension to our data, by also taking into account the square of the error value as well. Augmenting our data with our new feature results gives us the following plot.</p>
<div class="figure">
<img src="./resources/gauge_errors_augmented.png" alt="Figure 2: Pressure Gauge Errors Augmented With Squared Error" style="width:80.0%" />
<p class="caption"><strong>Figure 2</strong>: Pressure Gauge Errors Augmented With Squared Error</p>
</div>
<p>Now taking into account the squared value, the values farthest from zero will have larger second components than the values closer to zero, neatly separating our data in our artificial second dimension. So in this case, it was easy to see that creating a second component that consists of the square of the first will separate our data. So what did we actually do here? We created a <span class="math inline">\(feature map\)</span>, or a function that maps our original feature space into another higher-dimensional space. So all we need to do is apply our feature map to all of our data points and then we’re all set, right?</p>
<h3 id="improving-efficiency-with-kernels">Improving Efficiency with Kernels</h3>
<p>So we could apply our feature map to all of our data points, but can we do this more efficiently? Let’s take a closer look at our example. So right now our feature map looks like this: <span class="math inline">\(\phi(x)=(x, x^2)\)</span>, and looking at the equation for the perceptron, we take a dot product between an example to be classified and all of the training examples. This dot product computation <span class="math inline">\((x, x^2) * (y, y^2)\)</span> takes a total of 3 multiplies and an add (omitting the squaring for preprocessing), the procedure goes like this: <span class="math inline">\(x*y\)</span>, <span class="math inline">\(y^2\)</span>, <span class="math inline">\(x^2*y^2\)</span>. We can however, reorganize the equation to eliminate redundant computation: <span class="math inline">\(x*y + (x*y)^2\)</span>. With this equation we only need to compute the product <span class="math inline">\(x*y\)</span> once, reducing the total computation to 2 multiplies and an add. As an added benefit, we can reduce the memory footprint by half by no longer preprocessing our data! This is the kernel trick, and the closed form equation we found for the dot product is known as the <span class="math inline">\(kernel\)</span> function.</p>
<h3 id="the-polynomial-kernel">The Polynomial Kernel</h3>
<p>The feature map we constructed is actually a simpler version of a kernel that is well known, the polynomial kernel. It is defined as $K(x,y)=(x y + c)^d, where in our case <span class="math inline">\(d=2\)</span> so $K(x, y)= (xy + c)^2 =(xy)^2 + 2xyc + c = (x^2, x, ) (y^2, y, ). Which implies that our feature map is <span class="math inline">\(\phi(x)= (x^2, x\sqrt{2c}, \sqrt{c})\)</span> So as you can see, our kernel is essentially the polynomial kernel while ignoring the third constant dimension. This is an example of a kernel where preprocessing can become expensive if your feature space is high dimensional, because the implicit feature space dimension will combinatorially increase with the dimnension of your original feature space, while it will linearly increase with the parameter <span class="math inline">\(d\)</span>.</p>
<h3 id="finding-valid-kernel-functions">Finding Valid Kernel Functions</h3>
<p>The kernel function we found in the previous section was useful, but there are many others. How can we determine whether a given function is a valid kernel? In the previous section, we used our domain knowledge of our data to engineer our own feature map, <span class="math inline">\(\phi(x)=(x, x^2)\)</span>. To find the associated kernel <span class="math inline">\(K(x,y)\)</span>, we found a closed form for the expression <span class="math inline">\(\phi(x) \cdot \phi(y)\)</span>. This can always be done with any feature map that we construct ourselves. However, is there a way given a kernel <span class="math inline">\(K\)</span> to know whether it is a valid kernel? It so happens that there is a result from linear algebra called Mercer’s theorem. This theorem says that if you a have similarity function <span class="math inline">\(K(x,y)\)</span> that for any <span class="math inline">\(N\)</span> vectors</p>
<h3 id="kernel-perceptron">Kernel Perceptron</h3>
<h3 id="closing-thoughts">Closing Thoughts</h3>
<h3 id="extensions">Extensions</h3>
<ul>
<li>Lower the number of true joint samples we provide to the model during training, this would certainly increase the time taken to converge, but it would show off the main advantage of the model - that it can learn the joint distribution with a few paired samples.</li>
</ul>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<h3 id="explanation-of-perceptron-dual-equations">Explanation of Perceptron Dual Equations</h3>

                </div>
            </div>
        </div>
</body>
</html>
