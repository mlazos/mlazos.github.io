---
title: Reading List
---
## Reading List
This is a list of papers that I've found personally really useful to learn some key deep learning concepts from first principles. I recommend reading through them a few times and take notes on what the authors emphasize as important.

***

#### [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) 
This paper introduces the LSTM unit as a tool for sequence modeling. It provides a nice  motivation for the design of the LSTM.

***

#### [Overview of Computational Complexity for Gradient-Based Learning Algorithms](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf)
This provides a really nice survey of different gradient based learning methods and how they are derived from first principles.

***

#### [LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages](ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf)
This paper shows where LSTMs fit in the [Chomsky Hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy).

***

#### [Learning Phrase Representationsusing RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
This introduces the GRU and provides detail on the motivating scenario for the unit.

***

#### [Neural Machine Translation By Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
This introduced the concept of attention in neural models. Attention is a widely used technique that essentially models an importance function that will be applied to inputs in order to inform the model about which inputs are the most informative for its operation. 

***

#### [Hierarchical Attention Networks for Document Classification](http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
This is an example of a model which utilizes attention in  a straightforward way. This serves as an alternative example to [Neural Machine Translation By Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)

***