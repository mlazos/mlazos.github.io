<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <title>Mike Lazos - Reading List</title>

        <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css" />
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">

        <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
        <link rel="stylesheet" type="text/css" href="./css/custom.css" />
    </head>
    <body>
        <nav class="navbar navbar-default navbar-static-top">
            <div class="container-fluid">
                <div class="navbar-header">
                    <a class="navbar-brand" href="#">Mike Lazos</a>
                </div>
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav">
                        <li><a href="./index.html">Home</a></li>
                        <li><a href="./about.html">About</a></li>
                        <li><a href="./projects.html">Projects</a></li>
                        <li><a href="./posts.html">Posts</a></li>
                        <li><a href="./readings.html">Reading List</a></li>
                    </ul>
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="http://github.com/mlazos"><i class="fa fa-github fa-lg fa-fw"></i></a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <div class="container">
            <div class="row">
                <div class="col-md-10 col-md-offset-1">
                    <h2 id="reading-list">Reading List</h2>
<p>This is a list of papers that I’ve found personally really useful to learn some key deep learning concepts from first principles. I recommend reading through them a few times and take notes on what the authors emphasize.</p>
<hr />
<h4 id="long-short-term-memory"><a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory">Long Short-Term Memory</a></h4>
<p>This paper introduces the LSTM unit as a tool for sequence modeling. It provides a nice motivation for the design of the LSTM.</p>
<hr />
<h4 id="overview-of-computational-complexity-for-gradient-based-learning-algorithms"><a href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf">Overview of Computational Complexity for Gradient-Based Learning Algorithms</a></h4>
<p>This provides a really nice survey of different gradient based learning methods and how they are derived from first principles.</p>
<hr />
<h4 id="lstm-recurrent-networks-learn-simple-context-free-and-context-sensitive-languages"><a href="ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf">LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages</a></h4>
<p>This paper shows where LSTMs fit in the <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">Chomsky Hierarchy</a>.</p>
<hr />
<h4 id="learning-phrase-representationsusing-rnn-encoderdecoder-for-statistical-machine-translation"><a href="https://arxiv.org/pdf/1406.1078.pdf">Learning Phrase Representationsusing RNN Encoder–Decoder for Statistical Machine Translation</a></h4>
<p>This introduces the GRU and provides detail on the motivating scenario for the unit.</p>
<hr />
<h4 id="neural-machine-translation-by-jointly-learning-to-align-and-translate"><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation By Jointly Learning To Align and Translate</a></h4>
<p>This introduced the concept of attention in neural models. Attention is a widely used technique that essentially models an importance function that will be applied to inputs in order to inform the model about which inputs are the most informative for its operation.</p>
<hr />
<h4 id="hierarchical-attention-networks-for-document-classification"><a href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical Attention Networks for Document Classification</a></h4>
<p>This is an example of a model which utilizes attention in a straightforward way. This serves as an alternative example to <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation By Jointly Learning To Align and Translate</a></p>
<hr />
<h4 id="generative-adversarial-nets"><a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Nets</a></h4>
<p>This introduced Generative Adversarial Networks, a powerful tool for sampling from arbitrary distributions in an entirely unsupervised way.</p>
<hr />
                </div>
            </div>
        </div>
</body>
</html>
